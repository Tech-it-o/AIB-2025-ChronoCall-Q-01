from transformers import AutoModelForCausalLM, AutoTokenizer
import torch # ‡∏ï‡πâ‡∏≠‡∏á import torch ‡∏î‡πâ‡∏ß‡∏¢
import streamlit as st
import json

st.set_page_config(page_title="Test-ChronoCall-Q Output", page_icon="ü§ñ")
st.title("ChronoCall-Q Output")
st.caption("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Enter ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏∏‡πà‡∏° '‡∏™‡πà‡∏á'")

# model_name_or_path = "Qwen/Qwen3-0.6B"

# # ‡πÇ‡∏´‡∏•‡∏î TOOLS ‡∏à‡∏≤‡∏Å tools.json (‡∏Ñ‡∏ß‡∏£‡∏°‡∏µ‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏µ‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡πÑ‡∏î‡πÄ‡∏£‡∏Å‡∏ó‡∏≠‡∏£‡∏µ‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô)
# with open('tools.json', 'r', encoding='utf-8') as f:
#     TOOLS = json.load(f)

# # ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model (‡∏à‡∏∞‡∏ó‡∏≥‡πÅ‡∏Ñ‡πà‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ó‡∏≥‡∏á‡∏≤‡∏ô)
# @st.cache_resource # ‡πÉ‡∏ä‡πâ‡πÅ‡∏Ñ‡∏ä‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ã‡πâ‡∏≥‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä
# def load_model_and_tokenizer():
#     tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
#     model = AutoModelForCausalLM.from_pretrained(
#         model_name_or_path,
#         torch_dtype=torch.bfloat16, # ‡∏£‡∏∞‡∏ö‡∏∏ dtype ‡∏ó‡∏µ‡πà‡∏ä‡∏±‡∏î‡πÄ‡∏à‡∏ô
#         device_map="auto",
#         trust_remote_code=True,
#     )
#     return tokenizer, model

# tokenizer, model = load_model_and_tokenizer()

# def get_model_answer(messages):
#     # print("Model is running...") # ‡∏≠‡∏±‡∏ô‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏ô console ‡∏ó‡∏µ‡πà‡∏£‡∏±‡∏ô streamlit
#     text = tokenizer.apply_chat_template(
#         messages,
#         tokenize=False,
#         add_generation_prompt=True,
#         tools=TOOLS,
#         enable_thinking=False
#     )
#     inputs = tokenizer(text, return_tensors="pt").to(model.device)
#     # ‡πÉ‡∏ä‡πâ no_grad ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì gradient
#     with torch.no_grad():
#         outputs = model.generate(**inputs, max_new_tokens=512)
#     output_text = tokenizer.batch_decode(outputs)[0][len(text):]
#     return output_text

# # --- ‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡πÅ‡∏ä‡∏ó‡πÉ‡∏ô Streamlit ---

# # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ
# if "messages" not in st.session_state:
#     st.session_state.messages = [
#         {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\nCurrent Date: 2025-02-01.\n\nCurrent Day: Saturday."},
#         # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° system ‡∏ô‡∏µ‡πâ‡πÉ‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠ ‡πÅ‡∏ï‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
#     ]

# # ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
# for message in st.session_state.messages:
#     if message["role"] != "system": # ‡πÑ‡∏°‡πà‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° system ‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
#         with st.chat_message(message["role"]):
#             st.markdown(message["content"])

# # ‡∏ä‡πà‡∏≠‡∏á‡∏õ‡πâ‡∏≠‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
# if prompt := st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà..."):
#     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
#     st.session_state.messages.append({"role": "user", "content": prompt})
#     # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
#     with st.chat_message("user"):
#         st.markdown(prompt)

#     # ‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö
#     with st.spinner("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."): # ‡πÅ‡∏™‡∏î‡∏á‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏ß‡πà‡∏≤‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î
#         # ‡∏™‡πà‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏£‡∏±‡∏Å‡∏©‡∏≤ context
#         full_conversation_for_model = st.session_state.messages
#         response = get_model_answer(full_conversation_for_model)

#     # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤
#     st.session_state.messages.append({"role": "assistant", "content": response})
#     # ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ö‡∏ô‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠
#     with st.chat_message("assistant"):
#         st.markdown(response)