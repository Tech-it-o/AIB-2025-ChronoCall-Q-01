import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc # ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥

# --- 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ---
# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏ö‡∏ô CPU ‡∏´‡∏£‡∏∑‡∏≠ RAM ‡∏à‡∏≥‡∏Å‡∏±‡∏î: "Qwen/Qwen1.5-0.5B-Chat"
# ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ GPU ‡∏´‡∏£‡∏∑‡∏≠ RAM ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô: "Qwen/Qwen1.5-1.8B-Chat" ‡∏´‡∏£‡∏∑‡∏≠ "Qwen/Qwen1.5-7B-Chat"
MODEL_NAME = "Qwen/Qwen1.5-0.5B-Chat" # ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡πÑ‡∏î‡πâ‡∏ï‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£

# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡∏°‡∏µ GPU ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ precision
if torch.cuda.is_available():
    DEVICE = "cuda"
    DTYPE = torch.bfloat16 # ‡∏´‡∏£‡∏∑‡∏≠ torch.float16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU ‡∏ó‡∏µ‡πà‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö
    print(f"Using GPU: {DEVICE} with {DTYPE} precision")
else:
    DEVICE = "cpu"
    DTYPE = torch.float32 # CPU ‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö float32 ‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤
    print(f"Using CPU with {DTYPE} precision. Inference might be slow for larger models.")

@st.cache_resource
def load_qwen_model(model_name, device, dtype):
    """‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model ‡∏Ç‡∏≠‡∏á Qwen ‡πÅ‡∏•‡∏∞‡πÅ‡∏Ñ‡∏ä‡πÑ‡∏ß‡πâ"""
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else None, # "auto" ‡πÉ‡∏´‡πâ transformers ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ GPU
            # load_in_4bit=True # Uncomment ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î‡∏ô‡∏µ‡πâ‡πÅ‡∏•‡∏∞ pip install bitsandbytes ‡∏ñ‡πâ‡∏≤ RAM/VRAM ‡πÑ‡∏°‡πà‡∏û‡∏≠
        )
        if device == "cpu":
            model.to(device) # ‡∏¢‡πâ‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏õ CPU ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ device_map="auto"
        model.eval() # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô evaluation mode
        st.success(f"Model {model_name} loaded successfully on {device}.")
        return tokenizer, model
    except Exception as e:
        st.error(f"‚ö†Ô∏è Error loading model: {e}. Please check model name and available resources.")
        st.info("Try a smaller model like 'Qwen/Qwen1.5-0.5B-Chat' or enable `load_in_4bit=True` if you have `bitsandbytes` installed.")
        st.stop() # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Streamlit ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
tokenizer, model = load_qwen_model(MODEL_NAME, DEVICE, DTYPE)

# --- 2. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á Streamlit App ---
st.set_page_config(page_title="Qwen Chatbot")
st.title("ü§ñ Qwen Chatbot (Powered by Streamlit & Hugging Face)")
st.caption(f"üöÄ Using model: **{MODEL_NAME}** on **{DEVICE.upper()}**")

# --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° System message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen chat template
    st.session_state.messages.append({"role": "system", "content": "You are a helpful AI assistant."})
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å Assistant
    st.session_state.messages.append({"role": "assistant", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"})

# --- 4. ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏ô UI ---
for message in st.session_state.messages:
    # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á system message ‡πÉ‡∏ô UI ‡πÅ‡∏ï‡πà‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡πÉ‡∏ô session_state ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- 5. ‡∏£‡∏±‡∏ö Input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö ---
prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if prompt:
    # 5.1 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 5.2 ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô UI
    with st.chat_message("user"):
        st.markdown(prompt)

    # 5.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≤‡∏Å Qwen
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° List ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏£‡∏ß‡∏° system message)
        chat_history_for_model = []
        for msg in st.session_state.messages:
            chat_history_for_model.append({"role": msg["role"], "content": msg["content"]})

        # ‡πÉ‡∏ä‡πâ tokenizer.apply_chat_template ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen
        input_text = tokenizer.apply_chat_template(
            chat_history_for_model,
            tokenize=False,
            add_generation_prompt=True # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏≤ assistant ‡πÅ‡∏•‡πâ‡∏ß
        )

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_inputs = tokenizer([input_text], return_tensors="pt").to(DEVICE)

        try:
            # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
            generated_ids = model.generate(
                model_inputs.input_ids,
                max_new_tokens=512, # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                do_sample=True,    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
                top_p=0.8,         # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                temperature=0.7,   # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå
                repetition_penalty=1.0 # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å
            )

            # ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
            generated_response_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]
            bot_response = tokenizer.decode(generated_response_ids, skip_special_tokens=True)

            # Qwen ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏≠‡∏≤‡∏à‡∏™‡∏£‡πâ‡∏≤‡∏á output ‡∏ó‡∏µ‡πà‡∏°‡∏µ tag ‡∏´‡∏£‡∏∑‡∏≠ system message ‡∏ï‡∏¥‡∏î‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢
            # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
            if "<|im_end|>" in bot_response:
                bot_response = bot_response.split("<|im_end|>")[0].strip()
            if "<|im_start|>" in bot_response:
                 bot_response = bot_response.split("<|im_start|>")[-1].replace("assistant", "").strip()

            full_response = bot_response
            message_placeholder.markdown(full_response)

        except torch.cuda.OutOfMemoryError:
            st.error("GPU out of memory! Please try a smaller model or close other applications.")
            full_response = "Error: GPU out of memory. Cannot generate response."
            message_placeholder.markdown(full_response)
            torch.cuda.empty_cache()
            gc.collect() # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
        except Exception as e:
            st.error(f"An error occurred during generation: {e}")
            full_response = f"Error: {e}"
            message_placeholder.markdown(full_response)

    # 5.4 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Chatbot ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})