from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import streamlit as st
import json

st.set_page_config(page_title="Test-ChronoCall-Q", page_icon="ü§ñ")
st.title("ChronoCall-Q")
st.caption("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Enter ‡∏´‡∏£‡∏∑‡∏≠‡∏õ‡∏∏‡πà‡∏° '‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô'")

model_name_or_path = "TechitoTamani/Qwen3-0.6B_FinetuneWithMyData-Merged"

with open('tools.json', 'r', encoding='utf-8') as f:
    TOOLS = json.load(f)

@st.cache_resource
def load_model_and_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_name_or_path,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        trust_remote_code=True,
    )
    return tokenizer, model

tokenizer, model = load_model_and_tokenizer()

def get_model_answer(messages):
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        tools=TOOLS,
        enable_thinking=False
    )
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_new_tokens=512)
    output_text = tokenizer.batch_decode(outputs)[0][len(text):]
    return output_text

# --- Streamlit ---

st.markdown("""
    <style>
    /* ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏Å‡∏£‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏µ‡∏°‡πà‡∏ß‡∏á */
    div[data-baseweb="input"] > div {
        border: 2px solid #a020f0 !important;
        border-radius: 6px;
        padding: 2px;
        box-shadow: none !important;
    }

    /* ‡∏ï‡∏≠‡∏ô focus ‡πÅ‡∏•‡πâ‡∏ß */
    div[data-baseweb="input"] > div:focus-within {
        border: 2px solid #a020f0 !important;
        box-shadow: 0 0 0 2px rgba(160, 32, 240, 0.3) !important;
    }

    /* input ‡∏î‡πâ‡∏≤‡∏ô‡πÉ‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏á‡∏≤‡∏™‡∏µ‡πÅ‡∏î‡∏á‡πÄ‡∏•‡∏¢ */
    input {
        border: none !important;
        outline: none !important;
        box-shadow: none !important;
    }

    /* ‡∏•‡∏ö‡∏û‡∏ß‡∏Å‡∏Å‡∏£‡∏≠‡∏ö‡πÅ‡∏î‡∏á‡∏ó‡∏µ‡πà‡πÅ‡∏≠‡∏ö‡∏ã‡πà‡∏≠‡∏ô‡∏≠‡∏¢‡∏π‡πà */
    .css-1cpxqw2, .css-1d391kg, .css-1y4p8pa {
        border: 2px solid #a020f0 !important;
        box-shadow: none !important;
    }

    /* force ‡∏Å‡∏•‡πà‡∏≠‡∏á input ‡πÉ‡∏´‡πâ‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ‡∏™‡∏µ‡πÅ‡∏î‡∏á‡πÅ‡∏°‡πâ‡∏à‡∏∞ error */
    div:has(input:focus) {
        border-color: #a020f0 !important;
    }
    </style>
""", unsafe_allow_html=True)


if "user_input" not in st.session_state:
    st.session_state.user_input = ""

if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\nCurrent Date: 2025-02-01.\n\nCurrent Day: Saturday."},
    ]

user_input = st.text_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ñ‡∏≥‡∏™‡∏±‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà ‡πÅ‡∏•‡πâ‡∏ß‡∏Å‡∏î Enter ‡∏´‡∏£‡∏∑‡∏≠‡∏Å‡∏î‡∏õ‡∏∏‡πà‡∏°‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô", value=st.session_state.user_input, key="input")

submit_button = st.button("‡∏¢‡∏∑‡∏ô‡∏¢‡∏±‡∏ô")

if (user_input and user_input != st.session_state.user_input) or submit_button:

    st.session_state.messages.append({"role": "user", "content": user_input})

    with st.spinner("‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."):
        full_conversation_for_model = st.session_state.messages
        response = get_model_answer(full_conversation_for_model)

    st.session_state.messages.append({"role": "assistant", "content": response})

    st.success(f"Qwen: {response}")
    st.session_state.user_input = ""