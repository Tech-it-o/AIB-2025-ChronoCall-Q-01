import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import gc

# --- 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• ---
# ‡πÉ‡∏ä‡πâ Qwen1.5-0.5B-Chat ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# ‡πÅ‡∏•‡∏∞‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£ Deploy ‡∏ö‡∏ô Streamlit Cloud Free Tier
MODEL_NAME = "Qwen/Qwen1.5-0.5B-Chat" 

# ‡∏ö‡∏ô Streamlit Cloud ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô CPU ‡πÅ‡∏•‡∏∞‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏°‡∏µ RAM ‡∏à‡∏≥‡∏Å‡∏±‡∏î
# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 0.5B ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ 4-bit quantization ‡∏Å‡πá‡πÑ‡∏î‡πâ
# ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÅ‡∏≠‡∏õ‡∏¢‡∏±‡∏á Crash ‡∏´‡∏£‡∏∑‡∏≠‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏ä‡πâ‡∏≤‡∏°‡∏≤‡∏Å ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏ï‡∏±‡πâ‡∏á USE_4BIT = True
DEVICE = "cpu"
DTYPE = torch.float32 
USE_4BIT = False # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏î‡πâ‡∏ß‡∏¢ False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏Å‡∏ß‡πà‡∏≤ (‡∏ñ‡πâ‡∏≤ RAM ‡∏û‡∏≠)
                  # ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤ OOM ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô True ‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏¥‡πà‡∏° bitsandbytes ‡πÉ‡∏ô requirements.txt

print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {MODEL_NAME}")
print(f"‡πÉ‡∏ä‡πâ Device: {DEVICE}")
if USE_4BIT:
    print("‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö 4-bit quantization (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á bitsandbytes)")
else:
    print("‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ 4-bit quantization")

@st.cache_resource
def load_qwen_model(model_name, device, dtype, use_4bit):
    """
    ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model ‡∏Ç‡∏≠‡∏á Qwen ‡πÅ‡∏•‡∏∞‡πÅ‡∏Ñ‡∏ä‡πÑ‡∏ß‡πâ
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Quantization Config ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ 4-bit
        quantization_config = None
        if use_4bit:
            from transformers import BitsAndBytesConfig # import ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏•‡∏µ‡∏Å‡πÄ‡∏•‡∏µ‡πà‡∏¢‡∏á error ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4",
                bnb_4bit_compute_dtype=torch.bfloat16, 
                bnb_4bit_use_double_quant=True,
            )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else None, # ‡∏ö‡∏ô Streamlit Cloud ‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ cuda
            quantization_config=quantization_config, # ‡πÉ‡∏™‡πà quantization config ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        )

        model.eval() 
        
        st.success(f"‚úîÔ∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏• '{model_name}' ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ö‡∏ô {DEVICE.upper()} "
                   f"{'‡∏î‡πâ‡∏ß‡∏¢ 4-bit quantization' if use_4bit else ''}.")
        return tokenizer, model
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        st.info(f"‡πÇ‡∏°‡πÄ‡∏î‡∏• '{model_name}' ‡∏≠‡∏≤‡∏à‡∏¢‡∏±‡∏á‡πÉ‡∏´‡∏ç‡πà‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ ‡∏´‡∏£‡∏∑‡∏≠‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏Å‡∏≤‡∏£‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á")
        st.warning("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
        st.markdown("- **`MODEL_NAME`** ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
        st.markdown("- ‡∏´‡∏≤‡∏Å **`USE_4BIT = True`** ‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏ô‡πà‡πÉ‡∏à‡∏ß‡πà‡∏≤ **`bitsandbytes`** ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `requirements.txt`")
        st.markdown("- ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Ç‡∏≠‡∏á Streamlit Cloud ‡∏à‡∏£‡∏¥‡∏á‡πÜ")
        st.stop()

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
tokenizer, model = load_qwen_model(MODEL_NAME, DEVICE, DTYPE, USE_4BIT)

# --- 2. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á Streamlit App ---
st.set_page_config(page_title="Qwen Chatbot (Small)")
st.title("ü§ñ Qwen Chatbot")
st.caption(f"‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•: **{MODEL_NAME}** ‡∏ö‡∏ô **{DEVICE.upper()}** "
           f"{'‡∏î‡πâ‡∏ß‡∏¢ 4-bit quantization' if USE_4BIT else ''}")
st.write("‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö? (‡∏ô‡∏µ‡πà‡∏Ñ‡∏∑‡∏≠ Qwen ‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å)")

# --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° System message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen chat template
    st.session_state.messages.append({"role": "system", "content": "You are a helpful AI assistant."})
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å Assistant
    st.session_state.messages.append({"role": "assistant", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"})

# --- 4. ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏ô UI ---
for message in st.session_state.messages:
    if message["role"] != "system": # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á system message ‡πÉ‡∏ô UI
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- 5. ‡∏£‡∏±‡∏ö Input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö ---
prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if prompt:
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty() 
        full_response = ""
        
        chat_history_for_model = []
        for msg in st.session_state.messages:
            chat_history_for_model.append({"role": msg["role"], "content": msg["content"]})

        input_text = tokenizer.apply_chat_template(
            chat_history_for_model,
            tokenize=False,
            add_generation_prompt=True
        )

        model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."): 
            try:
                generated_ids = model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=512, 
                    do_sample=True,    
                    top_p=0.8,         
                    temperature=0.7,   
                    repetition_penalty=1.0 
                )

                generated_response_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]
                bot_response = tokenizer.decode(generated_response_ids, skip_special_tokens=True)

                if "<|im_end|>" in bot_response:
                    bot_response = bot_response.split("<|im_end|>")[0].strip()
                if "<|im_start|>assistant\n" in bot_response:
                    bot_response = bot_response.split("<|im_start|>assistant\n")[-1].strip()
                bot_response = bot_response.replace("<|im_start|>", "").replace("<|im_end|>", "").strip()

                full_response = bot_response
                message_placeholder.markdown(full_response)

            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {e}")
                full_response = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}. " \
                                "‡∏ô‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ö‡∏ô Streamlit Cloud."
                message_placeholder.markdown(full_response)
                gc.collect() 

    st.session_state.messages.append({"role": "assistant", "content": full_response})