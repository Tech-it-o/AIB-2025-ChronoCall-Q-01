import streamlit as st
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
import gc

# --- 1. ‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡∏∞‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏™‡πà‡∏ß‡∏ô‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç) ---
# **‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen3-4B**
# ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏ö‡∏ô Hugging Face Hub: https://huggingface.co/Qwen
# ‡∏´‡∏≤‡∏Å‡∏°‡∏µ‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô -Chat ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Chatbot ‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤
MODEL_NAME = "Qwen/Qwen3-4B-Chat" # <--- ‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏• Qwen3-4B-Chat ‡∏ó‡∏µ‡πà‡∏Ñ‡∏∏‡∏ì‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ (‡∏´‡∏£‡∏∑‡∏≠ Qwen/Qwen3-4B ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô Base)

# ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ precision ‡πÅ‡∏•‡∏∞ device
# ‡∏ö‡∏ô Streamlit Cloud ‡∏à‡∏∞‡πÄ‡∏õ‡πá‡∏ô CPU ‡πÅ‡∏•‡∏∞‡πÄ‡∏£‡∏≤‡∏à‡∏∞‡∏û‡∏∂‡πà‡∏á‡∏û‡∏≤ load_in_4bit
DEVICE = "cpu"
DTYPE = torch.float32 # ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ 4-bit quantization ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö DTYPE ‡∏ï‡∏£‡∏á‡πÜ ‡πÅ‡∏ï‡πà‡∏Ñ‡∏ß‡∏£‡∏ï‡∏±‡πâ‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô float32
USE_4BIT = True # <--- ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å: ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÅ‡∏ö‡∏ö 4-bit quantization ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö RAM ‡∏à‡∏≥‡∏Å‡∏±‡∏î

print(f"‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {MODEL_NAME}")
print(f"‡πÉ‡∏ä‡πâ Device: {DEVICE}")
if USE_4BIT:
    print("‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö 4-bit quantization (‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á bitsandbytes)")
else:
    print("‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡πÉ‡∏ä‡πâ 4-bit quantization (‡∏≠‡∏≤‡∏à‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ RAM/VRAM ‡∏™‡∏π‡∏á‡∏°‡∏≤‡∏Å)")

@st.cache_resource
def load_qwen_model(model_name, device, dtype, use_4bit):
    """
    ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡πÅ‡∏•‡∏∞ Model ‡∏Ç‡∏≠‡∏á Qwen ‡πÅ‡∏•‡∏∞‡πÅ‡∏Ñ‡∏ä‡πÑ‡∏ß‡πâ
    ‡πÉ‡∏ä‡πâ load_in_4bit ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    """
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î Quantization Config ‡∏ñ‡πâ‡∏≤‡πÉ‡∏ä‡πâ 4-bit
        quantization_config = None
        if use_4bit:
            quantization_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_quant_type="nf4", # NormalFloat 4-bit
                bnb_4bit_compute_dtype=torch.bfloat16, # ‡πÉ‡∏ä‡πâ bfloat16 ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏†‡∏≤‡∏¢‡πÉ‡∏ô
                bnb_4bit_use_double_quant=True, # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô double quantization
            )
        
        # ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=dtype,
            device_map="auto" if device == "cuda" else None,
            quantization_config=quantization_config, # ‡πÉ‡∏™‡πà quantization config ‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà
        )

        model.eval() # ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏õ‡πá‡∏ô evaluation mode
        
        st.success(f"‚úîÔ∏è ‡πÇ‡∏°‡πÄ‡∏î‡∏• '{model_name}' ‡πÇ‡∏´‡∏•‡∏î‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à‡∏ö‡∏ô {DEVICE.upper()} "
                   f"{'‡∏î‡πâ‡∏ß‡∏¢ 4-bit quantization' if use_4bit else ''}.")
        return tokenizer, model
    except Exception as e:
        st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•: {e}")
        st.info(f"**‡πÇ‡∏°‡πÄ‡∏î‡∏• '{model_name}' (4B) ‡∏°‡∏µ‡∏Ç‡∏ô‡∏≤‡∏î‡πÉ‡∏´‡∏ç‡πà‡∏°‡∏≤‡∏Å** ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Streamlit Cloud Free Tier.")
        st.warning("‡πÇ‡∏õ‡∏£‡∏î‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö:")
        st.markdown("- **`MODEL_NAME`** ‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏°‡πà")
        st.markdown("- **`USE_4BIT = True`** ‡πÅ‡∏•‡∏∞ **`bitsandbytes`** ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô `requirements.txt`")
        st.markdown("- ‡∏´‡∏≤‡∏Å‡∏¢‡∏±‡∏á‡∏Ñ‡∏á‡∏•‡πâ‡∏°‡πÄ‡∏´‡∏•‡∏ß, ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ô‡∏µ‡πâ‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏ô‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏Ç‡∏≠‡∏á Streamlit Cloud.")
        st.markdown("- ‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏•‡πá‡∏Å‡∏Å‡∏ß‡πà‡∏≤ (‡πÄ‡∏ä‡πà‡∏ô `Qwen/Qwen1.5-0.5B-Chat` ‡∏´‡∏£‡∏∑‡∏≠ `1.8B-Chat`) ‡∏´‡∏£‡∏∑‡∏≠ Deploy ‡∏ö‡∏ô Hugging Face Spaces (‡∏ó‡∏µ‡πà‡∏°‡∏µ GPU) ‡πÅ‡∏ó‡∏ô.")
        st.stop() # ‡∏´‡∏¢‡∏∏‡∏î‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á Streamlit ‡∏ñ‡πâ‡∏≤‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÅ‡∏≠‡∏õ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô
tokenizer, model = load_qwen_model(MODEL_NAME, DEVICE, DTYPE, USE_4BIT)

# --- 2. ‡∏™‡πà‡∏ß‡∏ô‡∏´‡∏±‡∏ß‡∏Ç‡∏≠‡∏á Streamlit App ---
st.set_page_config(page_title="Qwen3 Chatbot")
st.title("ü§ñ Qwen3 Chatbot")
st.caption(f"‡∏Ç‡∏±‡∏ö‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡πÇ‡∏î‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•: **{MODEL_NAME}** ‡∏ö‡∏ô **{DEVICE.upper()}** "
           f"{'‡∏î‡πâ‡∏ß‡∏¢ 4-bit quantization' if USE_4BIT else ''}")
st.write("‡∏•‡∏≠‡∏á‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏™‡∏ô‡∏ó‡∏ô‡∏≤!")

# --- 3. ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤ ---
if "messages" not in st.session_state:
    st.session_state.messages = []
    # ‡πÄ‡∏û‡∏¥‡πà‡∏° System message ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen chat template
    st.session_state.messages.append({"role": "system", "content": "You are a helpful AI assistant."})
    # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏ô‡∏£‡∏±‡∏ö‡∏à‡∏≤‡∏Å Assistant
    st.session_state.messages.append({"role": "assistant", "content": "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ‡∏Ñ‡∏£‡∏±‡∏ö! ‡∏°‡∏µ‡∏≠‡∏∞‡πÑ‡∏£‡πÉ‡∏´‡πâ‡∏ä‡πà‡∏ß‡∏¢‡πÑ‡∏´‡∏°‡∏Ñ‡∏£‡∏±‡∏ö?"})

# --- 4. ‡πÅ‡∏™‡∏î‡∏á‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏ô UI ---
for message in st.session_state.messages:
    if message["role"] != "system": # ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÅ‡∏™‡∏î‡∏á system message ‡πÉ‡∏ô UI
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# --- 5. ‡∏£‡∏±‡∏ö Input ‡∏à‡∏≤‡∏Å‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö ---
prompt = st.chat_input("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà...")
if prompt:
    # 5.1 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "user", "content": prompt})

    # 5.2 ‡πÅ‡∏™‡∏î‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô UI
    with st.chat_message("user"):
        st.markdown(prompt)

    # 5.3 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≤‡∏Å Qwen
    with st.chat_message("assistant"):
        message_placeholder = st.empty() # ‡πÉ‡∏ä‡πâ placeholder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏≠‡∏±‡∏õ‡πÄ‡∏î‡∏ï‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ
        full_response = ""
        
        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° List ‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏£‡∏ß‡∏° system message)
        chat_history_for_model = []
        for msg in st.session_state.messages:
            chat_history_for_model.append({"role": msg["role"], "content": msg["content"]})

        # ‡πÉ‡∏ä‡πâ tokenizer.apply_chat_template ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á prompt ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen
        input_text = tokenizer.apply_chat_template(
            chat_history_for_model,
            tokenize=False,
            add_generation_prompt=True # ‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏£‡∏π‡πâ‡∏ß‡πà‡∏≤‡∏ñ‡∏∂‡∏á‡∏ï‡∏≤ assistant ‡πÅ‡∏•‡πâ‡∏ß
        )

        # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° input ‡πÉ‡∏´‡πâ‡∏Å‡∏±‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•
        model_inputs = tokenizer([input_text], return_tensors="pt").to(model.device)

        with st.spinner("‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏¥‡∏î..."): # ‡πÄ‡∏û‡∏¥‡πà‡∏° spinner ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•
            try:
                # ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ï‡∏≠‡∏ö‡∏Å‡∏•‡∏±‡∏ö‡∏à‡∏≤‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
                generated_ids = model.generate(
                    model_inputs.input_ids,
                    max_new_tokens=512, # ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß‡∏™‡∏π‡∏á‡∏™‡∏∏‡∏î‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    do_sample=True,    # ‡πÄ‡∏õ‡∏¥‡∏î‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏∏‡πà‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢
                    top_p=0.8,         # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö
                    temperature=0.7,   # ‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏¥‡∏î‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏™‡∏£‡∏£‡∏Ñ‡πå
                    repetition_penalty=1.0 # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏ã‡πâ‡∏≥‡∏ã‡∏≤‡∏Å
                )

                # ‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô‡∏°‡∏≤‡πÉ‡∏´‡∏°‡πà‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                generated_response_ids = generated_ids[0][model_inputs.input_ids.shape[1]:]
                bot_response = tokenizer.decode(generated_response_ids, skip_special_tokens=True)

                # Qwen ‡∏ö‡∏≤‡∏á‡∏Ñ‡∏£‡∏±‡πâ‡∏á‡∏≠‡∏≤‡∏à‡∏™‡∏£‡πâ‡∏≤‡∏á output ‡∏ó‡∏µ‡πà‡∏°‡∏µ tag ‡∏´‡∏£‡∏∑‡∏≠ system message ‡∏ï‡∏¥‡∏î‡∏°‡∏≤‡∏î‡πâ‡∏ß‡∏¢
                # ‡πÄ‡∏£‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏ï‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ
                if "<|im_end|>" in bot_response:
                    bot_response = bot_response.split("<|im_end|>")[0].strip()
                # ‡∏Å‡∏£‡∏ì‡∏µ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á assistant role ‡∏ã‡πâ‡∏≥
                if "<|im_start|>assistant\n" in bot_response:
                    bot_response = bot_response.split("<|im_start|>assistant\n")[-1].strip()
                # ‡∏Å‡∏£‡∏ì‡∏µ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏à‡∏°‡∏µ artifact
                bot_response = bot_response.replace("<|im_start|>", "").replace("<|im_end|>", "").strip()


                full_response = bot_response
                message_placeholder.markdown(full_response)

            except Exception as e:
                st.error(f"‚ùå ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö: {e}")
                full_response = f"‡∏Ç‡∏≠‡∏≠‡∏†‡∏±‡∏¢‡∏Ñ‡∏£‡∏±‡∏ö ‡πÄ‡∏Å‡∏¥‡∏î‡∏Ç‡πâ‡∏≠‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•: {e}. " \
                                "‡∏ô‡∏µ‡πà‡∏≠‡∏≤‡∏à‡πÄ‡∏Å‡∏¥‡∏î‡∏à‡∏≤‡∏Å‡∏Ç‡∏µ‡∏î‡∏à‡∏≥‡∏Å‡∏±‡∏î‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥‡∏ö‡∏ô Streamlit Cloud."
                message_placeholder.markdown(full_response)
                gc.collect() # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÄ‡∏Ñ‡∏•‡∏µ‡∏¢‡∏£‡πå‡∏´‡∏ô‡πà‡∏ß‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥

    # 5.4 ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° Chatbot ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏ô‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥
    st.session_state.messages.append({"role": "assistant", "content": full_response})