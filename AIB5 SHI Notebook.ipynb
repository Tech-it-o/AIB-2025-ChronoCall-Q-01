{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Load the model","metadata":{}},{"cell_type":"code","source":"base_model = \"Qwen/Qwen3-0.6B\"\nadapter_path = \"TechitoTamani/Qwen3-0.6B_FinetuneWithMyData\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:02:55.248129Z","iopub.execute_input":"2025-06-01T02:02:55.248532Z","iopub.status.idle":"2025-06-01T02:02:55.252610Z","shell.execute_reply.started":"2025-06-01T02:02:55.248505Z","shell.execute_reply":"2025-06-01T02:02:55.251672Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\nimport torch\n\n# base_model = \"scb10x/typhoon2.1-gemma3-12b\"\n# adapter_path = \"/mnt/mydrive/Audio/outputs/typhoon_finetune_all_data_001/checkpoint-200\"\n\n# Load tokenizer and base model\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)  # to include any tokenizer special tokens\nmodel = AutoModelForCausalLM.from_pretrained(base_model, device_map=\"auto\", torch_dtype=torch.bfloat16)\n\n# Attach LoRA adapter\nmodel = PeftModel.from_pretrained(model, adapter_path)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:03:04.032425Z","iopub.execute_input":"2025-06-01T02:03:04.032727Z","iopub.status.idle":"2025-06-01T02:03:06.665116Z","shell.execute_reply.started":"2025-06-01T02:03:04.032705Z","shell.execute_reply":"2025-06-01T02:03:06.664416Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/config.py:162: UserWarning: Unexpected keyword arguments ['corda_config', 'trainable_token_indices'] for class LoraConfig, these are ignored. This probably means that you're loading a configuration file that was saved using a higher version of the library and additional parameters have been introduced since. It is highly recommended to upgrade the PEFT version before continuing (e.g. by running `pip install -U peft`).\n  warnings.warn(\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Merge the LoRA adapter weights into the model\nmodel = model.merge_and_unload()\n\n# Save the merged model\nsave_path = \"merged_model\"\nmodel.save_pretrained(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:03:20.746754Z","iopub.execute_input":"2025-06-01T02:03:20.747110Z","iopub.status.idle":"2025-06-01T02:03:23.554433Z","shell.execute_reply.started":"2025-06-01T02:03:20.747084Z","shell.execute_reply":"2025-06-01T02:03:23.553534Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(adapter_path)\ntokenizer.save_pretrained(save_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:03:28.209006Z","iopub.execute_input":"2025-06-01T02:03:28.209878Z","iopub.status.idle":"2025-06-01T02:03:29.031611Z","shell.execute_reply.started":"2025-06-01T02:03:28.209850Z","shell.execute_reply":"2025-06-01T02:03:29.030946Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"('merged_model/tokenizer_config.json',\n 'merged_model/special_tokens_map.json',\n 'merged_model/vocab.json',\n 'merged_model/merges.txt',\n 'merged_model/added_tokens.json',\n 'merged_model/tokenizer.json')"},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"## Run Inference","metadata":{}},{"cell_type":"code","source":"from vllm import LLM, SamplingParams\n\nllm = LLM(\n    model=\"merged_model\",  # Local directory with merged weights \n    max_model_len=1024,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:03:38.758454Z","iopub.execute_input":"2025-06-01T02:03:38.758764Z","iopub.status.idle":"2025-06-01T02:04:43.247899Z","shell.execute_reply.started":"2025-06-01T02:03:38.758742Z","shell.execute_reply":"2025-06-01T02:04:43.247013Z"}},"outputs":[{"name":"stdout","text":"WARNING 06-01 02:03:38 [config.py:3096] Your Tesla T4 device (with compute capability 7.5) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.\nWARNING 06-01 02:03:38 [config.py:3135] Casting torch.bfloat16 to torch.float16.\nINFO 06-01 02:03:38 [config.py:793] This model supports multiple tasks: {'score', 'embed', 'reward', 'generate', 'classify'}. Defaulting to 'generate'.\nINFO 06-01 02:03:38 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.0.1) with config: model='merged_model', speculative_config=None, tokenizer='merged_model', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=merged_model, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=True, pooler_config=None, compilation_config={\"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"cudagraph_capture_sizes\": [256, 248, 240, 232, 224, 216, 208, 200, 192, 184, 176, 168, 160, 152, 144, 136, 128, 120, 112, 104, 96, 88, 80, 72, 64, 56, 48, 40, 32, 24, 16, 8, 4, 2, 1], \"max_capture_size\": 256}, use_cached_outputs=False, \nINFO 06-01 02:03:40 [cuda.py:240] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\nINFO 06-01 02:03:40 [cuda.py:289] Using XFormers backend.\n","output_type":"stream"},{"name":"stderr","text":"[W601 02:03:51.007308638 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"name":"stdout","text":"INFO 06-01 02:04:01 [parallel_state.py:1064] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\nINFO 06-01 02:04:01 [model_runner.py:1170] Starting to load model merged_model...\n","output_type":"stream"},{"name":"stderr","text":"[W601 02:04:01.018116613 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"704741af589945bd9b2588fb6a5f9026"}},"metadata":{}},{"name":"stdout","text":"INFO 06-01 02:04:02 [default_loader.py:280] Loading weights took 0.88 seconds\nINFO 06-01 02:04:02 [model_runner.py:1202] Model loading took 1.1207 GiB and 0.997261 seconds\nINFO 06-01 02:04:04 [worker.py:291] Memory profiling takes 0.99 seconds\nINFO 06-01 02:04:04 [worker.py:291] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.90) = 13.27GiB\nINFO 06-01 02:04:04 [worker.py:291] model weights take 1.12GiB; non_torch_memory takes 0.02GiB; PyTorch activation peak memory takes 1.38GiB; the rest of the memory reserved for KV Cache is 10.74GiB.\nINFO 06-01 02:04:04 [executor_base.py:112] # cuda blocks: 6286, # CPU blocks: 2340\nINFO 06-01 02:04:04 [executor_base.py:117] Maximum concurrency for 1024 tokens per request: 98.22x\nINFO 06-01 02:04:10 [model_runner.py:1512] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9be4bbe2d4ca478fb33c472d621330c8"}},"metadata":{}},{"name":"stdout","text":"INFO 06-01 02:04:43 [model_runner.py:1670] Graph capturing finished in 33 secs, took 0.21 GiB\nINFO 06-01 02:04:43 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 40.56 seconds\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"sampling_params = SamplingParams(\n    temperature=0.0,\n    top_p=1.0,\n    max_tokens=128\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:05:37.504112Z","iopub.execute_input":"2025-06-01T02:05:37.504792Z","iopub.status.idle":"2025-06-01T02:05:37.508374Z","shell.execute_reply.started":"2025-06-01T02:05:37.504768Z","shell.execute_reply":"2025-06-01T02:05:37.507712Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"%%time\nprompt = \"วันนี้วันอะไร\"\noutputs = llm.generate(prompt, sampling_params)\nprint(outputs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:05:54.314100Z","iopub.execute_input":"2025-06-01T02:05:54.314404Z","iopub.status.idle":"2025-06-01T02:05:55.429417Z","shell.execute_reply.started":"2025-06-01T02:05:54.314379Z","shell.execute_reply":"2025-06-01T02:05:55.428466Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadc703a328641eca6311180970abedb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bf641af74e0435584dc7276405e2af9"}},"metadata":{}},{"name":"stdout","text":"[RequestOutput(request_id=2, prompt='วันนี้วันอะไร', prompt_token_ids=[37213, 66256, 20184, 124032, 37213, 66256, 128682], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text='บ้าง ตอนนี้มีนัดอะไรบ้าง ช่วยลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบท', token_ids=(36142, 124150, 220, 125634, 20184, 124032, 26283, 28319, 20184, 124090, 128682, 36142, 124150, 220, 48120, 126168, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829, 31782, 126829), cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1748743554.3248394, last_token_time=1748743555.4178486, first_scheduled_time=1748743554.3385594, first_token_time=1748743554.3966002, time_in_queue=0.013720035552978516, finished_time=1748743555.4179683, scheduler_time=0.010628970001107518, model_forward_time=None, model_execute_time=None, spec_token_acceptance_counts=[0]), lora_request=None, num_cached_tokens=0, multi_modal_placeholders={})]\nCPU times: user 1.11 s, sys: 14.3 ms, total: 1.13 s\nWall time: 1.11 s\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"outputs[0].outputs[0].text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-01T02:07:22.372022Z","iopub.execute_input":"2025-06-01T02:07:22.372661Z","iopub.status.idle":"2025-06-01T02:07:22.377698Z","shell.execute_reply.started":"2025-06-01T02:07:22.372637Z","shell.execute_reply":"2025-06-01T02:07:22.377066Z"}},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"'บ้าง ตอนนี้มีนัดอะไรบ้าง ช่วยลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบทลบท'"},"metadata":{}}],"execution_count":27}]}