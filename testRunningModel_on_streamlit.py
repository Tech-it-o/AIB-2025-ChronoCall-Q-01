from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import streamlit as st
import json
from peft import PeftModel, PeftConfig

st.set_page_config(page_title="Test-ChronoCall-Q Output", page_icon="ü§ñ")
st.title("Output ChronoCall-Q")
st.caption("‡∏û‡∏¥‡∏°‡∏û‡πå‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ ")

with open('tools.json', 'r', encoding='utf-8') as f:
    TOOLS = json.load(f)

# 1. ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ä‡∏∑‡πà‡∏≠ Base Model ‡πÅ‡∏•‡∏∞ LoRA ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì
base_model_id = "Qwen/Qwen3-0.6B" # ‡∏´‡∏£‡∏∑‡∏≠ "Qwen/Qwen3-0.6B-Chat"
lora_model_id = "TechitoTamani/Qwen3-0.6B_FinetuneWithMyData"

# 2. ‡πÇ‡∏´‡∏•‡∏î Tokenizer ‡∏Ç‡∏≠‡∏á Base Model
tokenizer = AutoTokenizer.from_pretrained(base_model_id)

# 3. ‡πÇ‡∏´‡∏•‡∏î Base Model
# ‡∏Å‡∏≥‡∏´‡∏ô‡∏î device ‡∏ó‡∏µ‡πà‡∏à‡∏∞‡πÉ‡∏ä‡πâ (cuda ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö GPU, cpu ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö CPU)
device = "cpu"
print(f"Using device: {device}")

# ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Qwen3 ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ trust_remote_code=True
model = AutoModelForCausalLM.from_pretrained(
    base_model_id,
    torch_dtype= torch.float32,
    device_map="auto", # ‡πÉ‡∏´‡πâ PEFT ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•
    trust_remote_code=True # Qwen ‡∏ö‡∏≤‡∏á‡πÄ‡∏ß‡∏≠‡∏£‡πå‡∏ä‡∏±‡∏ô‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ
)

# 4. ‡πÇ‡∏´‡∏•‡∏î LoRA ‡πÅ‡∏•‡∏∞‡∏ú‡∏ô‡∏ß‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡∏Å‡∏±‡∏ö Base Model
# ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏Ñ‡∏∏‡∏ì push ‡πÄ‡∏â‡∏û‡∏≤‡∏∞ LoRA ‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏õ LoRA ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∂‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏õ‡πá‡∏ô PEFT adapter
model = PeftModel.from_pretrained(model, lora_model_id)

def model_answer(messages):
    print("Model is running...") # This will still print to the console where Streamlit is run
    text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True,
        tools=TOOLS,
        enable_thinking=False
    )
    inputs = tokenizer(text, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=512)
    output_text = tokenizer.batch_decode(outputs)[0][len(text):]

    return(output_text)

if __name__ == "__main__":
    print("Testing the model...") # This will still print to the console where Streamlit is run
    messages = [
        {"role": "system", "content": "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n\nCurrent Date: 2025-02-01.\n\nCurrent Day: Saturday."},
        {"role": "user", "content": "‡πÄ‡∏≠‡∏≤‡∏ô‡∏±‡∏î‡∏ó‡∏≥‡∏Å‡∏≤‡∏£‡∏ö‡πâ‡∏≤‡∏ô‡∏Å‡∏±‡∏ö‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡∏û‡∏§‡∏´‡∏±‡∏™‡∏ó‡∏µ‡πà‡∏à‡∏∞‡∏ñ‡∏∂‡∏á‡∏ô‡∏µ‡πâ‡∏≠‡∏≠‡∏Å"},
    ]
    
    response = model_answer(messages)
    
    # Display the response in Streamlit
    st.write(response)
    # Or for a more prominent display:
    # st.success(response) # For a green success box
    # st.info(response) # For a blue info box
    # st.markdown(f"**Model Response:** {response}") # If you want to use Markdown for formatting